{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_cnn as data\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a dictionary with the labels of train and test\n",
      "Retrieving labels from the file /media/data/gomez/Fully_con_NN_Data_input/split1Labels.mat from train data\n",
      "Retrieving labels from the file /media/data/gomez/Fully_con_NN_Data_input/split1Labels.mat from test data\n",
      "Getting a dictionary with the data of train and test\n",
      "Retrieving data from the file /media/data/gomez/Fully_con_NN_Data_input/Input_data1.mat from train data\n",
      "Retrieving data from the file /media/data/gomez/Fully_con_NN_Data_input/Input_data1.mat from test data\n",
      "calculating max frame sequence in train and test data\n",
      "Maximum number of rows found and it is: 28\n",
      "from train data\n",
      "from test data\n"
     ]
    }
   ],
   "source": [
    "dictLabels = data.getAllLabels('/media/data/gomez/Fully_con_NN_Data_input/split1Labels.mat')\n",
    "dictData = data.getAllData('/media/data/gomez/Fully_con_NN_Data_input/Input_data1.mat')\n",
    "\n",
    "dataTrain = dictData['train']\n",
    "dataTest = dictData['test']\n",
    "labelsTrain = dictLabels['train']\n",
    "labelsTest = dictLabels['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parameters of training and viewing probabilities of the \n",
    "training_epochs = 10\n",
    "drop_out_prob = 0.5\n",
    "display_step = 10\n",
    "iterations = 9000\n",
    "learning_rate = 0.00001\n",
    "tb_logs_path = '/home/gomez/tb_net_result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 840 12 227 227\n"
     ]
    }
   ],
   "source": [
    "batch_size,shpeData = dataTrain.shape\n",
    "_,shpeLabl = labelsTrain.shape\n",
    "print('training', shpeData,shpeLabl,batch_size,_)\n",
    "'''batch_size,shpeData = dataTest.shape\n",
    "_,shpeLabl = labelsTest.shape\n",
    "print('test',shpeData,shpeLabl,batch_size,_)'''\n",
    "\n",
    "#Modify the data according to the given shape above\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,840])\n",
    "    y_ = tf.placeholder(tf.float32,[None,12])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight initialization\n",
    "\n",
    "def weight_variable(shape, name, var_type): #Change the type in order to specify the variable to use    \n",
    "    with tf.name_scope('weight_' + name):\n",
    "        initial = tf.random_uniform(shape)\n",
    "        if(var_type == 'normal'):\n",
    "            initial = tf.random_normal(shape, stddev=0.1)\n",
    "        elif(var_type == 'trunc'): #As seen in the beginning\n",
    "            initial = tf.truncated_normal(shape, stddev=0.1)            \n",
    "        w_variable = tf.Variable(initial)\n",
    "        tf.scalar_summary('_weight_' + name, w_variable)\n",
    "        return w_variable\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    with tf.name_scope('bias_' + name):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        b_variable = tf.Variable(initial)\n",
    "        tf.scalar_summary('_bias_' + name,b_variable)\n",
    "        return b_variable\n",
    "\n",
    "#Convolution and pooling (Martin: stride 30 for convolution, article: max_pooling of 7 with stride 7)\n",
    "\n",
    "def conv1d(x,w):\n",
    "    with tf.name_scope('Conv1d'):\n",
    "        convolution = tf.nn.conv1d(x,w,stride=30, padding='SAME')    \n",
    "        tf.scalar_summary('_Conv1d_',convolution)\n",
    "        return convolution\n",
    "\n",
    "def max_pool_7(x):\n",
    "    with tf.name_scope('max_pool_stride7'):\n",
    "        max_pool7 = tf.nn.max_pool(x,ksize=[1,1,7,1],strides=[1,1,7,1], padding='SAME') #There is just max_pool in 2d and 3d so reshape and reshape\n",
    "        tf.scalar_summary('_max_pool_stride7_',max_pool7)\n",
    "        return max_pool7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(?, 28, 32), dtype=float32)\n",
      "Tensor(\"max_pool_stride7/MaxPool:0\", shape=(?, 1, 4, 1), dtype=float32)\n",
      "Tensor(\"Reshape_2:0\", shape=(?, 4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#First Layer\n",
    "\n",
    "w_conv1 = weight_variable([30,1,32], 'conv1', 'normal') #Kernel size of 30 and from 1 input to 32 neurons\n",
    "b_conv1 = bias_variable([32], 'conv1') #32 biases going to different neurons\n",
    "\n",
    "x_data = tf.reshape(x, [-1,shpeData,1]) #Image converted in 3d\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv1d(x_data, w_conv1) + b_conv1)\n",
    "\n",
    "print(h_conv1)\n",
    "\n",
    "'''data: [3d] -> maxpooling(data): [4d] -> reconversion: [3d]'''\n",
    "\n",
    "# h_conv1_4d:[batch_size,1 (1d data), 28 (840/30 = after convolution 30 (filter and stride) ), 32 (the output to the new network)]\n",
    "\n",
    "h_conv1_4d = tf.reshape(h_conv1,[-1,1,int(shpeData/30),1]) #Error is here --> Sure 32 is here???\n",
    "\n",
    "h_pool1 = max_pool_7(h_conv1_4d)\n",
    "\n",
    "print(h_pool1)\n",
    "\n",
    "#2 rshp_h_pool1:[batch_size, 4 (28/7 -> 28 from previous convolution/ 7 from maxpool filter and stride), 32 (output network)]\n",
    "\n",
    "rshp_h_pool1 = tf.reshape(h_pool1,[-1,int((shpeData/30)/7),1]) #Error is here --> Sure 32 is here???\n",
    "print(rshp_h_pool1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_3:0\", shape=(?, 128), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Fully Connected layer 1\n",
    "\n",
    "W_fc1 = weight_variable([4 * 32,128], 'fc1','uniform') #weights converted from the dimensions\n",
    "b_fc1 = bias_variable([128], 'fc1')\n",
    "\n",
    "h_pool2_flat = tf.reshape(rshp_h_pool1, [-1, 4*32])\n",
    "print(h_pool2_flat)\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "print(h_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu_2:0\", shape=(?, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Fully Connected layer 2\n",
    "\n",
    "W_fc2 = weight_variable([128,256], 'fc2', 'uniform') #weights converted to \n",
    "b_fc2 = bias_variable([256], 'fc2') #Bias values going to the neurons\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1,W_fc2)+b_fc2) #Relu #2 -> Check if this was right -> Should be (before the h_pool2 was)\n",
    "print(h_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout/mul_1:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(?, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Dropout trick \n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2,keep_prob)\n",
    "print(h_fc2_drop)\n",
    "\n",
    "# Output layer\n",
    "\n",
    "W_fc3 = weight_variable([256,12], 'fc3', 'uniform')\n",
    "b_fc3 = bias_variable([12], 'fc3')\n",
    "\n",
    "eps = tf.constant(0.00001, shape=[12]) #Constant value added to prevent underflow (probability of having zero terms)\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc2_drop, W_fc3) + b_fc3) + eps\n",
    "print(y_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ScalarSummary_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train and evaluate\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.arg_max(y_conv,1),tf.arg_max(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "tf.scalar_summary('cost', cross_entropy)\n",
    "tf.scalar_summary('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merging all summaries\n",
    "summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'init = tf.initialize_all_variables()\\nsess = tf.InteractiveSession()\\nsess.run(init)\\nwriter = tf.train.SummaryWriter(tb_logs_path, graph=tf.get_default_graph())'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Starting a new session\n",
    "'''init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.train.SummaryWriter(tb_logs_path, graph=tf.get_default_graph())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration: 2270 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 4540 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 6810 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 9080 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 11350 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 13620 , loss: 2.543833 Accuracy: 0.074890\n",
      " Iteration: 15890 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 18160 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 20430 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 22700 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 24970 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 27240 , loss: 2.570271 Accuracy: 0.048458\n",
      " Iteration: 29510 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 31780 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 34050 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 36320 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 38590 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 40860 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 43130 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 45400 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 47670 , loss: 2.517289 Accuracy: 0.101322\n",
      " Iteration: 49940 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 52210 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 54480 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 56750 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 59020 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 61290 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 63560 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 65830 , loss: 2.508535 Accuracy: 0.110132\n",
      " Iteration: 68100 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 70370 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 72640 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 74910 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 77180 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 79450 , loss: 2.530572 Accuracy: 0.088106\n",
      " Iteration: 81720 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 83990 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 86260 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 88530 , loss: 2.504189 Accuracy: 0.114537\n",
      " Iteration: 90800 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 93070 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 95340 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 97610 , loss: 2.539297 Accuracy: 0.079295\n",
      " Iteration: 99880 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 102150 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 104420 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 106690 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 108960 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 111230 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 113500 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 115770 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 118040 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 120310 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 122580 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 124850 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 127120 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 129390 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 131660 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 133930 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 136200 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 138470 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 140740 , loss: 2.535010 Accuracy: 0.083700\n",
      " Iteration: 143010 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 145280 , loss: 2.512890 Accuracy: 0.105727\n",
      " Iteration: 147550 , loss: 2.517385 Accuracy: 0.101322\n",
      " Iteration: 149820 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 152090 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 154360 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 156630 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 158900 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 161170 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 163440 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 165710 , loss: 2.526217 Accuracy: 0.092511\n",
      " Iteration: 167980 , loss: 2.530530 Accuracy: 0.088106\n",
      " Iteration: 170250 , loss: 2.535016 Accuracy: 0.083700\n",
      " Iteration: 172520 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 174790 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 177060 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 179330 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 181600 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 183870 , loss: 2.508596 Accuracy: 0.110132\n",
      " Iteration: 186140 , loss: 2.523561 Accuracy: 0.096916\n",
      " Iteration: 188410 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 190680 , loss: 2.535041 Accuracy: 0.083700\n",
      " Iteration: 192950 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 195220 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 197490 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 199760 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 202030 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 204300 , loss: 2.540436 Accuracy: 0.079295\n",
      " Iteration: 206570 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 208840 , loss: 2.517398 Accuracy: 0.101322\n",
      " Iteration: 211110 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 213380 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 215650 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 217920 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 220190 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 222460 , loss: 2.490974 Accuracy: 0.127753\n",
      " Iteration: 224730 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 227000 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 229270 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 231540 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 233810 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 236080 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 238350 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 240620 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 242890 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 245160 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 247430 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 249700 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 251970 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 254240 , loss: 2.535022 Accuracy: 0.083700\n",
      " Iteration: 256510 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 258780 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 261050 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 263320 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 265590 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 267860 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 270130 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 272400 , loss: 2.521756 Accuracy: 0.096916\n",
      " Iteration: 274670 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 276940 , loss: 2.530608 Accuracy: 0.088106\n",
      " Iteration: 279210 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 281480 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 283750 , loss: 2.473355 Accuracy: 0.145374\n",
      " Iteration: 286020 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 288290 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 290560 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 292830 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 295100 , loss: 2.548156 Accuracy: 0.070485\n",
      " Iteration: 297370 , loss: 2.543828 Accuracy: 0.074890\n",
      " Iteration: 299640 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 301910 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 304180 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 306450 , loss: 2.539391 Accuracy: 0.079295\n",
      " Iteration: 308720 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 310990 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 313260 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 315530 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 317800 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 320070 , loss: 2.526660 Accuracy: 0.092511\n",
      " Iteration: 322340 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 324610 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 326880 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 329150 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 331420 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 333690 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 335960 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 338230 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 340500 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 342770 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 345040 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 347310 , loss: 2.517332 Accuracy: 0.101322\n",
      " Iteration: 349580 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 351850 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 354120 , loss: 2.574676 Accuracy: 0.044053\n",
      " Iteration: 356390 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 358660 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 360930 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 363200 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 365470 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 367740 , loss: 2.504191 Accuracy: 0.114537\n",
      " Iteration: 370010 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 372280 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 374550 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 376820 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 379090 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 381360 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 383630 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 385900 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 388170 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 390440 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 392710 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 394980 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 397250 , loss: 2.561352 Accuracy: 0.057269\n",
      " Iteration: 399520 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 401790 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 404060 , loss: 2.562956 Accuracy: 0.057269\n",
      " Iteration: 406330 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 408600 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 410870 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 413140 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 415410 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 417680 , loss: 2.512866 Accuracy: 0.105727\n",
      " Iteration: 419950 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 422220 , loss: 2.482165 Accuracy: 0.136564\n",
      " Iteration: 424490 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 426760 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 429030 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 431300 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 433570 , loss: 2.521808 Accuracy: 0.096916\n",
      " Iteration: 435840 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 438110 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 440380 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 442650 , loss: 2.539426 Accuracy: 0.079295\n",
      " Iteration: 444920 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 447190 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 449460 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 451730 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 454000 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 456270 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 458540 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 460810 , loss: 2.521811 Accuracy: 0.096916\n",
      " Iteration: 463080 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 465350 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 467620 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 469890 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 472160 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 474430 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 476700 , loss: 2.513003 Accuracy: 0.105727\n",
      " Iteration: 478970 , loss: 2.521805 Accuracy: 0.096916\n",
      " Iteration: 481240 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 483510 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 485780 , loss: 2.513003 Accuracy: 0.105727\n",
      " Iteration: 488050 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 490320 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 492590 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 494860 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 497130 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 499400 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 501670 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 503940 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 506210 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 508480 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 510750 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 513020 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 515290 , loss: 2.526095 Accuracy: 0.092511\n",
      " Iteration: 517560 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 519830 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 522100 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 524370 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 526640 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 528910 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 531180 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 533450 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 535720 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 537990 , loss: 2.504186 Accuracy: 0.114537\n",
      " Iteration: 540260 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 542530 , loss: 2.512001 Accuracy: 0.105727\n",
      " Iteration: 544800 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 547070 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 549340 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 551610 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 553880 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 556150 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 558420 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 560690 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 562960 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 565230 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 567500 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 569770 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 572040 , loss: 2.539541 Accuracy: 0.079295\n",
      " Iteration: 574310 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 576580 , loss: 2.534985 Accuracy: 0.083700\n",
      " Iteration: 578850 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 581120 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 583390 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 585660 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 587930 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 590200 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 592470 , loss: 2.534925 Accuracy: 0.083700\n",
      " Iteration: 594740 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 597010 , loss: 2.517272 Accuracy: 0.101322\n",
      " Iteration: 599280 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 601550 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 603820 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 606090 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 608360 , loss: 2.499619 Accuracy: 0.118943\n",
      " Iteration: 610630 , loss: 2.499767 Accuracy: 0.118943\n",
      " Iteration: 612900 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 615170 , loss: 2.535021 Accuracy: 0.083700\n",
      " Iteration: 617440 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 619710 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 621980 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 624250 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 626520 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 628790 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 631060 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 633330 , loss: 2.539426 Accuracy: 0.079295\n",
      " Iteration: 635600 , loss: 2.477760 Accuracy: 0.140969\n",
      " Iteration: 637870 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 640140 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 642410 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 644680 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 646950 , loss: 2.526217 Accuracy: 0.092511\n",
      " Iteration: 649220 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 651490 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 653760 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 656030 , loss: 2.526215 Accuracy: 0.092511\n",
      " Iteration: 658300 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 660570 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 662840 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 665110 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 667380 , loss: 2.526196 Accuracy: 0.092511\n",
      " Iteration: 669650 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 671920 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 674190 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 676460 , loss: 2.570271 Accuracy: 0.048458\n",
      " Iteration: 678730 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 681000 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 683270 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 685540 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 687810 , loss: 2.543827 Accuracy: 0.074890\n",
      " Iteration: 690080 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 692350 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 694620 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 696890 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 699160 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 701430 , loss: 2.508581 Accuracy: 0.110132\n",
      " Iteration: 703700 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 705970 , loss: 2.526216 Accuracy: 0.092511\n",
      " Iteration: 708240 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 710510 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 712780 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 715050 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 717320 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 719590 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 721860 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 724130 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 726400 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 728670 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 730940 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 733210 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 735480 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 737750 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 740020 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 742290 , loss: 2.508596 Accuracy: 0.110132\n",
      " Iteration: 744560 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 746830 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 749100 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 751370 , loss: 2.510131 Accuracy: 0.110132\n",
      " Iteration: 753640 , loss: 2.521800 Accuracy: 0.096916\n",
      " Iteration: 755910 , loss: 2.543820 Accuracy: 0.074890\n",
      " Iteration: 758180 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 760450 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 762720 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 764990 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 767260 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 769530 , loss: 2.539428 Accuracy: 0.079295\n",
      " Iteration: 771800 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 774070 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 776340 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 778610 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 780880 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 783150 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 785420 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 787690 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 789960 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 792230 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 794500 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 796770 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 799040 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 801310 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 803580 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 805850 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 808120 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 810390 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 812660 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 814930 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 817200 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 819470 , loss: 2.513003 Accuracy: 0.105727\n",
      " Iteration: 821740 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 824010 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 826280 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 828550 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 830820 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 833090 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 835360 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 837630 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 839900 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 842170 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 844440 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 846710 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 848980 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 851250 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 853520 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 855790 , loss: 2.486571 Accuracy: 0.132159\n",
      " Iteration: 858060 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 860330 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 862600 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 864870 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 867140 , loss: 2.535017 Accuracy: 0.083700\n",
      " Iteration: 869410 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 871680 , loss: 2.539423 Accuracy: 0.079295\n",
      " Iteration: 873950 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 876220 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 878490 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 880760 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 883030 , loss: 2.543818 Accuracy: 0.074890\n",
      " Iteration: 885300 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 887570 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 889840 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 892110 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 894380 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 896650 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 898920 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 901190 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 903460 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 905730 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 908000 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 910270 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 912540 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 914810 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 917080 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 919350 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 921620 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 923890 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 926160 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 928430 , loss: 2.508467 Accuracy: 0.110132\n",
      " Iteration: 930700 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 932970 , loss: 2.544406 Accuracy: 0.074890\n",
      " Iteration: 935240 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 937510 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 939780 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 942050 , loss: 2.521812 Accuracy: 0.096916\n",
      " Iteration: 944320 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 946590 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 948860 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 951130 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 953400 , loss: 2.548233 Accuracy: 0.070485\n",
      " Iteration: 955670 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 957940 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 960210 , loss: 2.490974 Accuracy: 0.127753\n",
      " Iteration: 962480 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 964750 , loss: 2.512997 Accuracy: 0.105727\n",
      " Iteration: 967020 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 969290 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 971560 , loss: 2.539382 Accuracy: 0.079295\n",
      " Iteration: 973830 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 976100 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 978370 , loss: 2.548175 Accuracy: 0.070485\n",
      " Iteration: 980640 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 982910 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 985180 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 987450 , loss: 2.548236 Accuracy: 0.070485\n",
      " Iteration: 989720 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 991990 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 994260 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 996530 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 998800 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1001070 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1003340 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1005610 , loss: 2.504152 Accuracy: 0.114537\n",
      " Iteration: 1007880 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1010150 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1012420 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1014690 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1016960 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1019230 , loss: 2.508303 Accuracy: 0.110132\n",
      " Iteration: 1021500 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1023770 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1026040 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1028310 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1030580 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1032850 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1035120 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1037390 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1039660 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1041930 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1044200 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1046470 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1048740 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1051010 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1053280 , loss: 2.574676 Accuracy: 0.044053\n",
      " Iteration: 1055550 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1057820 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1060090 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1062360 , loss: 2.519402 Accuracy: 0.101322\n",
      " Iteration: 1064630 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1066900 , loss: 2.513003 Accuracy: 0.105727\n",
      " Iteration: 1069170 , loss: 2.526217 Accuracy: 0.092511\n",
      " Iteration: 1071440 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1073710 , loss: 2.530500 Accuracy: 0.088106\n",
      " Iteration: 1075980 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1078250 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 1080520 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1082790 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1085060 , loss: 2.504181 Accuracy: 0.114537\n",
      " Iteration: 1087330 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1089600 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1091870 , loss: 2.530542 Accuracy: 0.088106\n",
      " Iteration: 1094140 , loss: 2.548125 Accuracy: 0.070485\n",
      " Iteration: 1096410 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 1098680 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 1100950 , loss: 2.504204 Accuracy: 0.114537\n",
      " Iteration: 1103220 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1105490 , loss: 2.548238 Accuracy: 0.070485\n",
      " Iteration: 1107760 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1110030 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1112300 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1114570 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1116840 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1119110 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 1121380 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1123650 , loss: 2.508588 Accuracy: 0.110132\n",
      " Iteration: 1125920 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1128190 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1130460 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1132730 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1135000 , loss: 2.552643 Accuracy: 0.066079\n",
      " Iteration: 1137270 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1139540 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1141810 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1144080 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1146350 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1148620 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1150890 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1153160 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1155430 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1157700 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1159970 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1162240 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1164510 , loss: 2.543738 Accuracy: 0.074890\n",
      " Iteration: 1166780 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1169050 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1171320 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1173590 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1175860 , loss: 2.546908 Accuracy: 0.070485\n",
      " Iteration: 1178130 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 1180400 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1182670 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1184940 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1187210 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1189480 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1191750 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1194020 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1196290 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1198560 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1200830 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1203100 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1205370 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 1207640 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1209910 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1212180 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1214450 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1216720 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1218990 , loss: 2.486571 Accuracy: 0.132159\n",
      " Iteration: 1221260 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1223530 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1225800 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1228070 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1230340 , loss: 2.548243 Accuracy: 0.070485\n",
      " Iteration: 1232610 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1234880 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1237150 , loss: 2.486571 Accuracy: 0.132159\n",
      " Iteration: 1239420 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1241690 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1243960 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1246230 , loss: 2.517271 Accuracy: 0.101322\n",
      " Iteration: 1248500 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1250770 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 1253040 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1255310 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1257580 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1259850 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1262120 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1264390 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1266660 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1268930 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1271200 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1273470 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1275740 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1278010 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1280280 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1282550 , loss: 2.504107 Accuracy: 0.114537\n",
      " Iteration: 1284820 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1287090 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1289360 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1291630 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1293900 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1296170 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1298440 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1300710 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1302980 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1305250 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1307520 , loss: 2.539429 Accuracy: 0.079295\n",
      " Iteration: 1309790 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1312060 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1314330 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1316600 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 1318870 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1321140 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1323410 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1325680 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1327950 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1330220 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1332490 , loss: 2.521734 Accuracy: 0.096916\n",
      " Iteration: 1334760 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1337030 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1339300 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1341570 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1343840 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1346110 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1348380 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1350650 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1352920 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1355190 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1357460 , loss: 2.535025 Accuracy: 0.083700\n",
      " Iteration: 1359730 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1362000 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1364270 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1366540 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1368810 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1371080 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1373350 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1375620 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1377890 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1380160 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1382430 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1384700 , loss: 2.543762 Accuracy: 0.074890\n",
      " Iteration: 1386970 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 1389240 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1391510 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 1393780 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1396050 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1398320 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1400590 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 1402860 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1405130 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 1407400 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1409670 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1411940 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1414210 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1416480 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1418750 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1421020 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1423290 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1425560 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1427830 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1430100 , loss: 2.517410 Accuracy: 0.101322\n",
      " Iteration: 1432370 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1434640 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1436910 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1439180 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1441450 , loss: 2.508582 Accuracy: 0.110132\n",
      " Iteration: 1443720 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1445990 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1448260 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1450530 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1452800 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1455070 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1457340 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1459610 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1461880 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1464150 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1466420 , loss: 2.513821 Accuracy: 0.105727\n",
      " Iteration: 1468690 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1470960 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1473230 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1475500 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1477770 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 1480040 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1482310 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1484580 , loss: 2.535040 Accuracy: 0.083700\n",
      " Iteration: 1486850 , loss: 2.539379 Accuracy: 0.079295\n",
      " Iteration: 1489120 , loss: 2.570289 Accuracy: 0.048458\n",
      " Iteration: 1491390 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1493660 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1495930 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1498200 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1500470 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1502740 , loss: 2.539435 Accuracy: 0.079295\n",
      " Iteration: 1505010 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1507280 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1509550 , loss: 2.539432 Accuracy: 0.079295\n",
      " Iteration: 1511820 , loss: 2.548140 Accuracy: 0.070485\n",
      " Iteration: 1514090 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1516360 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 1518630 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1520900 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 1523170 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1525440 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1527710 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 1529980 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1532250 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1534520 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1536790 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 1539060 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1541330 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1543600 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1545870 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1548140 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1550410 , loss: 2.517396 Accuracy: 0.101322\n",
      " Iteration: 1552680 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 1554950 , loss: 2.521801 Accuracy: 0.096916\n",
      " Iteration: 1557220 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1559490 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1561760 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1564030 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1566300 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1568570 , loss: 2.508594 Accuracy: 0.110132\n",
      " Iteration: 1570840 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1573110 , loss: 2.510835 Accuracy: 0.105727\n",
      " Iteration: 1575380 , loss: 2.530491 Accuracy: 0.088106\n",
      " Iteration: 1577650 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1579920 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1582190 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1584460 , loss: 2.517282 Accuracy: 0.101322\n",
      " Iteration: 1586730 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1589000 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1591270 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1593540 , loss: 2.534997 Accuracy: 0.083700\n",
      " Iteration: 1595810 , loss: 2.490976 Accuracy: 0.127753\n",
      " Iteration: 1598080 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1600350 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1602620 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1604890 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1607160 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1609430 , loss: 2.510163 Accuracy: 0.110132\n",
      " Iteration: 1611700 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1613970 , loss: 2.543784 Accuracy: 0.074890\n",
      " Iteration: 1616240 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1618510 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1620780 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 1623050 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1625320 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1627590 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1629860 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1632130 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1634400 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1636670 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1638940 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1641210 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1643480 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1645750 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1648020 , loss: 2.521812 Accuracy: 0.096916\n",
      " Iteration: 1650290 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1652560 , loss: 2.530622 Accuracy: 0.088106\n",
      " Iteration: 1654830 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1657100 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1659370 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1661640 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1663910 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1666180 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1668450 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1670720 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1672990 , loss: 2.477760 Accuracy: 0.140969\n",
      " Iteration: 1675260 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1677530 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1679800 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1682070 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1684340 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1686610 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1688880 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1691150 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1693420 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1695690 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1697960 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1700230 , loss: 2.512993 Accuracy: 0.105727\n",
      " Iteration: 1702500 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1704770 , loss: 2.539415 Accuracy: 0.079295\n",
      " Iteration: 1707040 , loss: 2.565866 Accuracy: 0.052863\n",
      " Iteration: 1709310 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1711580 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1713850 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1716120 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1718390 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1720660 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1722930 , loss: 2.499787 Accuracy: 0.118943\n",
      " Iteration: 1725200 , loss: 2.526217 Accuracy: 0.092511\n",
      " Iteration: 1727470 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1729740 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 1732010 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1734280 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1736550 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1738820 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1741090 , loss: 2.517341 Accuracy: 0.101322\n",
      " Iteration: 1743360 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1745630 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1747900 , loss: 2.548241 Accuracy: 0.070485\n",
      " Iteration: 1750170 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1752440 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1754710 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1756980 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1759250 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1761520 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1763790 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1766060 , loss: 2.517304 Accuracy: 0.101322\n",
      " Iteration: 1768330 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1770600 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1772870 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1775140 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1777410 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1779680 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1781950 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1784220 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1786490 , loss: 2.495381 Accuracy: 0.123348\n",
      " Iteration: 1788760 , loss: 2.533820 Accuracy: 0.083700\n",
      " Iteration: 1791030 , loss: 2.543838 Accuracy: 0.074890\n",
      " Iteration: 1793300 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1795570 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 1797840 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1800110 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1802380 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1804650 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1806920 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1809190 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1811460 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1813730 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1816000 , loss: 2.543833 Accuracy: 0.074890\n",
      " Iteration: 1818270 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1820540 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1822810 , loss: 2.583487 Accuracy: 0.035242\n",
      " Iteration: 1825080 , loss: 2.570271 Accuracy: 0.048458\n",
      " Iteration: 1827350 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1829620 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1831890 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1834160 , loss: 2.530624 Accuracy: 0.088106\n",
      " Iteration: 1836430 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1838700 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1840970 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1843240 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1845510 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1847780 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1850050 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1852320 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1854590 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1856860 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1859130 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1861400 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1863670 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1865940 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1868210 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1870480 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1872750 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1875020 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1877290 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1879560 , loss: 2.539342 Accuracy: 0.079295\n",
      " Iteration: 1881830 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1884100 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1886370 , loss: 2.548245 Accuracy: 0.070485\n",
      " Iteration: 1888640 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1890910 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1893180 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1895450 , loss: 2.508597 Accuracy: 0.110132\n",
      " Iteration: 1897720 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1899990 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1902260 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1904530 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1906800 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 1909070 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1911340 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1913610 , loss: 2.521809 Accuracy: 0.096916\n",
      " Iteration: 1915880 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1918150 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1920420 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1922690 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1924960 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1927230 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1929500 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1931770 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1934040 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1936310 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1938580 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1940850 , loss: 2.535026 Accuracy: 0.083700\n",
      " Iteration: 1943120 , loss: 2.535023 Accuracy: 0.083700\n",
      " Iteration: 1945390 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1947660 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1949930 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 1952200 , loss: 2.460139 Accuracy: 0.158590\n",
      " Iteration: 1954470 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 1956740 , loss: 2.486548 Accuracy: 0.132159\n",
      " Iteration: 1959010 , loss: 2.517407 Accuracy: 0.101322\n",
      " Iteration: 1961280 , loss: 2.486571 Accuracy: 0.132159\n",
      " Iteration: 1963550 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1965820 , loss: 2.504192 Accuracy: 0.114537\n",
      " Iteration: 1968090 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 1970360 , loss: 2.548244 Accuracy: 0.070485\n",
      " Iteration: 1972630 , loss: 2.561460 Accuracy: 0.057269\n",
      " Iteration: 1974900 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 1977170 , loss: 2.539416 Accuracy: 0.079295\n",
      " Iteration: 1979440 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1981710 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1983980 , loss: 2.513004 Accuracy: 0.105727\n",
      " Iteration: 1986250 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 1988520 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 1990790 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 1993060 , loss: 2.552650 Accuracy: 0.066079\n",
      " Iteration: 1995330 , loss: 2.557036 Accuracy: 0.061674\n",
      " Iteration: 1997600 , loss: 2.499786 Accuracy: 0.118943\n",
      " Iteration: 1999870 , loss: 2.521813 Accuracy: 0.096916\n",
      " Iteration: 2002140 , loss: 2.521902 Accuracy: 0.096916\n",
      " Iteration: 2004410 , loss: 2.513005 Accuracy: 0.105727\n",
      " Iteration: 2006680 , loss: 2.517408 Accuracy: 0.101322\n",
      " Iteration: 2008950 , loss: 2.530623 Accuracy: 0.088106\n",
      " Iteration: 2011220 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 2013490 , loss: 2.557055 Accuracy: 0.061674\n",
      " Iteration: 2015760 , loss: 2.526218 Accuracy: 0.092511\n",
      " Iteration: 2018030 , loss: 2.535027 Accuracy: 0.083700\n",
      " Iteration: 2020300 , loss: 2.535028 Accuracy: 0.083700\n",
      " Iteration: 2022570 , loss: 2.530619 Accuracy: 0.088106\n",
      " Iteration: 2024840 , loss: 2.535029 Accuracy: 0.083700\n",
      " Iteration: 2027110 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 2029380 , loss: 2.539434 Accuracy: 0.079295\n",
      " Iteration: 2031650 , loss: 2.543839 Accuracy: 0.074890\n",
      " Iteration: 2033920 , loss: 2.517405 Accuracy: 0.101322\n",
      " Iteration: 2036190 , loss: 2.513000 Accuracy: 0.105727\n",
      " Iteration: 2038460 , loss: 2.513002 Accuracy: 0.105727\n",
      " Iteration: 2040730 , loss: 2.530623 Accuracy: 0.088106\n",
      "Optimization Finished\n",
      "And test accuracy is: 0.11236\n"
     ]
    }
   ],
   "source": [
    "'''for i in range(iterations):    \n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:dataTrain, y_:labelsTrain, keep_prob: 0.5})\n",
    "        #summary = summary_op.eval(feed_dict={x:dataTrain, y_:labelsTrain, keep_prob: 0.5})\n",
    "        print(\"step %d, training accuracy %g\"%(i,train_accuracy))\n",
    "        #writer.add_summary(summary, epoch)\n",
    "    train_step.run(feed_dict={x:dataTrain, y_:labelsTrain, keep_prob: 0.5})\n",
    "    #writer.add_summary\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x:dataTest, y_:labelsTest, keep_prob: 0.5}))'''\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step < iterations:\n",
    "        sess.run(train_step, feed_dict={x:dataTrain, y_:labelsTrain, keep_prob: drop_out_prob})\n",
    "        if step % display_step == 0:\n",
    "            #Calculates batch accuracy\n",
    "            train_acc, loss = sess.run([accuracy, cost], feed_dict={x:dataTrain, y_:labelsTrain, keep_prob: drop_out_prob})\n",
    "            print(' Iteration: ' + str(step*batch_size) + ' , loss: ' + '{:.6f}'.format(loss) + ' Accuracy: ' + '{:.6f}'.format(train_acc))\n",
    "        step +=1\n",
    "    print('Optimization Finished')\n",
    "    \n",
    "    test_acc = sess.run(accuracy, feed_dict={x:dataTest, y_:labelsTest, keep_prob:drop_out_prob})\n",
    "    print('And test accuracy is:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
